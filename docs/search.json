[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lane Following using Pure Pursuit Control",
    "section": "",
    "text": "This project implements lane following on a Duckiebot using trajectory generation and pure pursuit control. The standard Duckietown lane-following stack estimates lane pose (d, \\phi) via a histogram filter and applies a PID controller on that estimate. In practice, the (d, \\phi) measurement can be noisy and discontinuous. Especially under partial occlusions, sparse lane markings, or during turns, which leads to jittery steering and reduced driving smoothness.\nInstead of using (d, \\phi), we use perception output (ground-projected lane segments) directly for local path construction. At each time step, we generate a centerline trajectory in the robot frame, apply temporal smoothing, and track the resulting path using pure pursuit control. This replaces explicit lane-state estimation with a path-tracking formulation: control commands are computed directly from the local trajectory, yielding smoother driving through curves.\nThe baseline stack consists of:\n\nLine detection: color segmentation (white/yellow/red), edge detection (Canny), and line extraction (Hough) to produce lane-marker segments in the image.\nGround projection: homography-based mapping from image coordinates to the ground plane using camera calibration.\nLane estimation: a histogram filter over (d, \\phi) that fuses a kinematic prediction with segment-based updates.\nControl: PID-style steering using the estimated (d, \\phi), typically with constant forward velocity and angular correction based on lateral and heading error.\n\nWe keep line detection and ground projection, but replace the downstream modules:\n\nTrajectory generation: compute a forward centerline path from projected segments, with robust fitting and temporal smoothing.\nPure pursuit control: select a lookahead goal point on the path and compute curvature/steering commands to track it.\n\n\n\n\n\n\nflowchart LR\n  A[\"Camera Image\"] --&gt; B[\"Line Detection\"]\n  B --&gt; C[\"Ground Projection\"]\n\n  subgraph Baseline_Duckietown [\"Baseline Duckietown Stack\"]\n    D[\"Lane Estimation\"] --&gt; E[\"PID Control\"]\n  end\n\n  subgraph Our_Project [\"Our Project Stack\"]\n    F[\"Trajectory Generation\"] --&gt; G[\"Pure Pursuit Control\"]\n  end\n\n  C --&gt; D\n  C --&gt; F\n  E --&gt; H[\"Wheel Commands\"]\n  G --&gt; H\n\n\n\n\n\n\nOver this project we set and achieved these three goals:\n\nTrajectory generation from detected lane segments, including temporal smoothing for stability.\nPure pursuit control on the generated trajectory.\nDemonstrate continuous looped driving on a real Duckiebot in the lab Duckietown."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Lane Following using Pure Pursuit Control",
    "section": "",
    "text": "This project implements lane following on a Duckiebot using trajectory generation and pure pursuit control. The standard Duckietown lane-following stack estimates lane pose (d, \\phi) via a histogram filter and applies a PID controller on that estimate. In practice, the (d, \\phi) measurement can be noisy and discontinuous. Especially under partial occlusions, sparse lane markings, or during turns, which leads to jittery steering and reduced driving smoothness.\nInstead of using (d, \\phi), we use perception output (ground-projected lane segments) directly for local path construction. At each time step, we generate a centerline trajectory in the robot frame, apply temporal smoothing, and track the resulting path using pure pursuit control. This replaces explicit lane-state estimation with a path-tracking formulation: control commands are computed directly from the local trajectory, yielding smoother driving through curves.\nThe baseline stack consists of:\n\nLine detection: color segmentation (white/yellow/red), edge detection (Canny), and line extraction (Hough) to produce lane-marker segments in the image.\nGround projection: homography-based mapping from image coordinates to the ground plane using camera calibration.\nLane estimation: a histogram filter over (d, \\phi) that fuses a kinematic prediction with segment-based updates.\nControl: PID-style steering using the estimated (d, \\phi), typically with constant forward velocity and angular correction based on lateral and heading error.\n\nWe keep line detection and ground projection, but replace the downstream modules:\n\nTrajectory generation: compute a forward centerline path from projected segments, with robust fitting and temporal smoothing.\nPure pursuit control: select a lookahead goal point on the path and compute curvature/steering commands to track it.\n\n\n\n\n\n\nflowchart LR\n  A[\"Camera Image\"] --&gt; B[\"Line Detection\"]\n  B --&gt; C[\"Ground Projection\"]\n\n  subgraph Baseline_Duckietown [\"Baseline Duckietown Stack\"]\n    D[\"Lane Estimation\"] --&gt; E[\"PID Control\"]\n  end\n\n  subgraph Our_Project [\"Our Project Stack\"]\n    F[\"Trajectory Generation\"] --&gt; G[\"Pure Pursuit Control\"]\n  end\n\n  C --&gt; D\n  C --&gt; F\n  E --&gt; H[\"Wheel Commands\"]\n  G --&gt; H\n\n\n\n\n\n\nOver this project we set and achieved these three goals:\n\nTrajectory generation from detected lane segments, including temporal smoothing for stability.\nPure pursuit control on the generated trajectory.\nDemonstrate continuous looped driving on a real Duckiebot in the lab Duckietown."
  },
  {
    "objectID": "index.html#trajectory-generation",
    "href": "index.html#trajectory-generation",
    "title": "Lane Following using Pure Pursuit Control",
    "section": "Trajectory Generation",
    "text": "Trajectory Generation\nThis module uses ground-projected lane detections in the robot frame: white points \\mathcal{W}={(x_i,y_i)} and yellow points \\mathcal{Y}={(x_j,y_j)}, where x is forward distance and y is lateral offset. The goal is to output a local centerline trajectory \\Gamma={(x_k, y_k)}_{k=1}^{N} over a fixed horizon x_k \\in [0, \\texttt{max\\_forward}], suitable for pure pursuit. In practice, the key challenge is that the point sets are noisy (mainly due to false detections), so the trajectory generator must be robust and temporally stable.\n\nMethods explored but discarded\n1) Midpoint pairing heuristic. We first attempted to construct the centerline by pairing boundary points and taking midpoints. Concretely, for each detected yellow point we found the nearest white point and defined the midpoint of the pair as a centerline sample. This fails on real hardware because white detections frequently “bleed” onto the yellow marker (e.g., due to lighting and segmentation artifacts). As a result, nearest-neighbor pairing becomes unreliable: midpoints drift toward a boundary and jump between inconsistent pairings across frames. The downstream fit then produces trajectories that are not representative of the lane center.\n2) Bézier fit + normal shift. We also tried fitting a Bézier curve to lane detections and shifting it inward by half the lane width using the curve normal. While geometrically appealing, this approach is brittle because the Bézier fit tends to be strongly influenced by individual detections. With sparse or noisy points, a single outlier can significantly change the curve shape, again producing unreliable trajectories and also with large frame-to-frame variations.\n\n\nFinal approach: polynomial fitting with RANSAC\nWhat worked reliably was a robust polynomial fit in the ground plane, using RANSAC to reject outliers.\n\nChoose which boundary to fit. In our experiments, fitting on white points was most stable (more consistent and typically more numerous). We default to \\mathcal{W}, and switch to \\mathcal{Y} only when there are insufficient white detections.\n\n\n\n\n\n\n\nCautionWhite Point Filtering\n\n\n\n\n\nBecause the system is configured for right-hand driving, we sometimes detect white points from the adjacent (left) lane boundary. To prevent mixing these into the fit, we discard white points that lie too far to the left (beyond roughly half a lane width). This simple geometric filter significantly reduced “lane switching” behavior in the fitted curve.\n\n\n\n\nFit a polynomial model robustly. We fit y(x) as a quadratic polynomial and use RANSAC so that spurious points (from floor, glare, or wrong-color detections) do not dominate the model. Intuitively, RANSAC repeatedly fits the curve to small random subsets and keeps the hypothesis that agrees with the largest set of points. Once these inliers are identified, the final curve is fit on all the inliers to ensure maximum accuracy. This gives a curve that follows the dominant lane boundary even when the input is noisy.\nConvert boundary curve to centerline. Once we have a clean boundary curve, we compute the local normal direction along the curve and shift laterally by half the lane width to obtain the centerline.\nSample a fixed-horizon trajectory. We uniformly sample N points in x from 0 to \\texttt{max\\_forward} and evaluate the centerline model to produce the final trajectory points (x_k,y_k). This produces a consistent representation for pure pursuit (same number of points, same horizon, every frame).\n\n\n\nTemporal buffering for smoothness and consistency\nA good single-frame trajectory is not enough as pure pursuit can be sensitive to frame-to-frame trajectory jumps, which show up as steering jitter. We therefore stabilize the trajectory over time with two mechanisms:\n1) Reject implausible trajectory flips. We compute a simple “heading proxy” from the trajectory midpoint (a point around the middle of the horizon). If the direction from the robot to this midpoint changes by more than a threshold \\theta_{\\text{thresh}} compared to the previous frame, we treat the current trajectory as unreliable and reuse the previous one. An example of the \\theta computation is also shown in Figure 1. This catches cases where the fit snaps to the wrong structure for a frame (common under sparse or false detections).\n\n\n\n\n\n\nFigure 1: Heading change calulcation between the current and previous trajectory\n\n\n\n2) Exponential smoothing of the lateral profile. When a trajectory is accepted, we still smooth it to reduce high-frequency noise: \ny^{\\text{sm}}_{t} = \\alpha*y_{t} + (1-\\alpha)*y^{\\text{sm}}_{t-1},\n\napplied pointwise across the sampled trajectory. The parameter \\alpha controls responsiveness vs. smoothness. This simple filter substantially reduces steering jitter while preserving curvature for cornering within the short planning horizon.\nOverall, this pipeline produces a locally defined, robust, and temporally stable centerline trajectory from ground-projected lane points, which is then tracked by pure pursuit.\nThe code for trajectory generation can be found here"
  },
  {
    "objectID": "index.html#pure-pursuit-control",
    "href": "index.html#pure-pursuit-control",
    "title": "Lane Following using Pure Pursuit Control",
    "section": "Pure Pursuit Control",
    "text": "Pure Pursuit Control\nThe second part of the project focused on enhancing the controller node using the “Pure Pursuit” algorithm, which enables the robot to adjust its path before the error accumulates. Similar to PID control, pure pursuit is a steering method that computes the linear (v) and angular velocity (\\omega). However, instead of relying on the cross-track error (CTE), the lateral distance between the vehicle and the centerlane, it relies on the pre-computed trajectory as a reference.\nThe pure pursuit controller consists of two keys steps:\n\nGoal Point Computation: We determine the goal point to which the robot aims to reach\nControl Commands Computation: We compute the linear and angular velocity required to make the duckie reach the computed goal point\n\n\n\n\n\n\nflowchart LR\n    A[Goal Point Computation] --&gt; B[Control Command Computation]\n\n\n\n\n\n\nIf you are familiar with the “follow the carrot” analogy, pure pursuit control works fashionably in the same way: we make the robot (the donkey) move towards the goal point (the carrot), which we always keep at a distance L_d, the lookahead distance. If the donkey is too far from the goal point, we “stick” it to the robot and make it turn more aggressively towards the goal point.\n\n\nGoal Point Calculation - Line-circle intersection algorithm\nMany methods can be used to compute the goal point, but we decided to go with the “line-circle intersection” algorithm for its computational simplicity.\nMathematically speaking, the line-circle intersection algorithm tries to find the points where a straight line intersects the circle with radius R.\n\n\n\n\n\n\nNoteUnderstanding the maths behind the line-circle intersection algorithm\n\n\n\n\n\nGiven a circle centered at the origin with radius r and points P1(x1, y1) and P2(x2, y2). The implicit line equation for the closed-form geometry can be written as\n d_y x - d_x y + D = 0\nwhere\n\nD=x_1 y_2 - x_2 y_1\nd_x = x_2 - x_1\nd_y = y_2 - y_1\n\nThe perpendicular projection of the origin onto the line is:\n\n\\begin{aligned}\nx_0 &= \\frac{D d_y}{d_r^2} \\\\\ny_0 &= \\frac{-D d_x}{d_r^2}\n\\end{aligned}\n\nwhere dr= \\sqrt{dx^2 + dy^2}\nSince the closest point from the origin to the line is a perpendicular projection, we can determine the existence of the intersection using the distance from the origin to the line\n\\text{dist} = \\frac{|D|}{d_r}\n\nIf dist &gt; r, there are no intersection\nIf dist = r, one tangent intersection\nIf dist &lt; r, there are two intersections\n\nFrom the right-triangle geometry, we can find the distance along the line from the closest point to each intersection, which we can use to move from the closest point forward and backward along the line\nh = \\sqrt{r^2 - \\left(\\frac{D}{d_r}\\right)^2}\nThe unit direction along the line is given by\n\\hat{\\mathbf{u}} = \\left( \\frac{d_x}{d_r}, \\frac{d_y}{d_r} \\right)\nThis gives us the formula to compute the intesection points\n\n\\mathbf{P}_{1,2}\n=\n\\begin{pmatrix}\nx_0 \\\\\ny_0\n\\end{pmatrix}\n\\pm\nh \\,\\hat{\\mathbf{u}}\n\n\n\\begin{aligned}\nx &= \\frac{D d_y \\pm d_x \\sqrt{r^2 d_r^2 - D^2}}{d_r^2} \\\\\ny &= \\frac{-D d_x \\pm d_y \\sqrt{r^2 d_r^2 - D^2}}{d_r^2}\n\\end{aligned}\n\n\n\n\nSince the line-circle intersection method works for two points only and the trajectory is an array of points, given two consecutive trajectory points P1(x1, y2) and P2(x2, y2), the goal point P(x, y) can be found as follows:\n\nx=\\frac{D dy \\pm sgn(dy) dx \\sqrt{\\Delta}}{L_d^2}\ny=\\frac{-D dx \\pm \\| dy \\| \\sqrt{\\Delta}}{L_d^2}\n\nWhere\n\ndx=x_2-x_1\ndy=y_2-y_1\ndr= \\sqrt{dx^2 + dy^2}\nD=x_1 y_2 - x_2 y_1\n\\Delta=r^2 dr^2 - D^2\nf(x) = \\begin{cases} 0, & x&lt;0 \\\\ x, & x\\ge 0 \\end{cases}\n\nTo determine the validity of the goal point computed, we can follow the following graph:\n\n\n\n\n\nflowchart LR\n    A[Discriminant] -- $$\\Delta$$&lt;0 --&gt; B(No intersection Found) --&gt; E;\n    A -- $$\\Delta$$=0 --&gt; C(One Intersection Found);\n    A -- $$\\Delta$$&gt;0 --&gt; D(Two Intersections Found);\n    E[Use last point in trajectory];\n    F(Range Check);\n    G[Select intersection closest to last goal point];\n    C -- invalid --&gt; E;\n    D -- both invalid --&gt; E;\n    D -- one invalid --&gt; C;\n    D -- both valid --&gt; G;\n\n\n\n\n\n\nEssentially, we compute the discriminant \\Delta to find valid intersections. The intersection we find is valid if \\Delta \\ge 0 and if it’s between the trajectory segment points.\nThe full implementation can be found inside the function find_goal_point()\n\n\nControl Command Computation - “Follow the carrot” approach\nThe first implementation of pure pursuit that we coded kept the linear velocity constant and only accounted for the angular velocity, which was calculated using the turn error between the robot heading and the goal point.\nWe define 3 parameters:\n\nlookahead_distance: Circle radius at which the duckiebot sees\nkp_steering: How hard do we want to steer the wheel upon turn error\nv_bar: Linear velocity\n\nConsider the following picture,\n\nThe turn error can be computed\n\\alpha = tan(\\frac{y_1-y_0}{x_1 - x_0})\nThus,\n\\omega = \\text{kp\\_sterring} \\cdot \\alpha\nThe code for the “follow the carrot” approach looks something like this:\n\n\nCode\ngoal_point, _ = find_goal_point(\n    path_points,\n    self.current_pos,\n    lookahead_distance,\n    self.last_found_index,\n)\ndx, dy = (\n    goal_point[0] - self.current_pos[0],\n    goal_point[1] - self.current_pos[1],\n)\nabs_target_angle = math.atan2(dy, dx)\nturn_error = abs_target_angle - np.deg2rad(self.current_heading)\nL_d = math.sqrt(dx**2 + dy**2)\n\nv = v_bar\nomega = kp_steering * turn_error\n\n\nThe full code implementation can be found in the function compute_control_action()\n\n\nControl Command Computation - Tangent approach\nIn order to improve the duckiebot performance around sharp corners, we wanted to adjust the linear velocity and angle velocity based on curvature. Similarly to how one would drive, we want our duckie to slow down around corner and speed up on when it drives straight ahead. To do so, we used a tangent based approach to find the curvature.\nConsider the following picture:\n\n\n\nCourtesy of Purdue Sigbots\n\n\nUsing trigonometry, we find that\nR = \\frac{L_d}{2 sin(\\alpha)}\n\n\n\n\n\n\nNoteDeriving the radius formula\n\n\n\n\n\nAs seen previously, given the current pos P0(x0, y0) and the goal point P1(x1, y1), the turn error \\alpha can be computed as\n\\alpha = tan(\\frac{y_1 - y_0}{x_1 - x_0})\nSince the goal point is at distance L_d of the current position, the half-way point is at distance \\frac{L_d}{2} and thus\nR = \\frac{\\frac{L_d}{2}}{sin(\\alpha)} = \\frac{L_d}{2 sin (\\alpha)}\n\n\n\nThe curvature k is defined as the inverse of the radius of the arc we want the duckie to travel. Because we want the linear velocity to be inversely proportional to the curvature (the higher the curvature, the lower the speed), we get the following formula for linear velocity\nk = \\frac{1}{R} v = \\bar{v} * \\frac{1}{k} = \\bar{v} * R\nTo derive the angular error, now consider the diagram below:\n\n\n\nCourtesy of Purdue Sigbots\n\n\nAssuming that the robot finishes the turn after time \\delta t, we have that the left side (L_l) and right side (L_R) of the duckie turn with velocity\n\n\\begin{aligned}\nL_l\n&=\n(\\text{linearVel} - \\text{turnVel}) \\cdot \\Delta t\n=\n\\left( R - \\frac{W}{2} \\right) \\cdot (2 \\cdot \\text{turnError})\n\\\\[6pt]\nL_r\n&=\n(\\text{linearVel} + \\text{turnVel}) \\cdot \\Delta t\n=\n\\left( R + \\frac{W}{2} \\right) \\cdot (2 \\cdot \\text{turnError})\n\\end{aligned}\n\nSolving for turn velocity, we get that\n\n\\frac{(\\text{linearVel} - \\text{turnVel}) \\cdot \\Delta t}\n     {(\\text{linearVel} + \\text{turnVel}) \\cdot \\Delta t}\n=\n\\frac{\\left( R - \\frac{W}{2} \\right) \\cdot (2 \\cdot \\text{turnError})}\n     {\\left( R + \\frac{W}{2} \\right) \\cdot (2 \\cdot \\text{turnError})}\n\n\n\\frac{\\text{linearVel} - \\text{turnVel}}\n     {\\text{linearVel} + \\text{turnVel}}\n=\n\\frac{R - \\frac{W}{2}}\n     {R + \\frac{W}{2}}\n\n\n\\left( R + \\frac{W}{2} \\right)\\text{linearVel}\n-\n\\left( R + \\frac{W}{2} \\right)\\text{turnVel}\n=\n\\left( R - \\frac{W}{2} \\right)\\text{linearVel}\n+\n\\left( R - \\frac{W}{2} \\right)\\text{turnVel}\n\n\n\\text{turnVel}\n=\n\\frac{W}{2R} \\cdot \\text{linearVel}\n\nAnd since R = \\frac{\\text{lookaheadDistance}}{2 \\sin(\\text{turnError})}, the turn velocity\n\n\\boxed{\n\\text{turnVel}\n=\n\\frac{W \\sin(\\text{turnError})}\n     {\\text{lookaheadDistance}}\n\\cdot \\text{linearVel}\n}\n\nThus, our curvature-based pure pursuit approach uses 4 parameters:\n\nwidth: width of the duckiebot chassis\nomega_factor: how hard we want to turn the steering wheel, very similar to kp_steering\nv_bar: default velocity\nv_bar_min: minimal linear velocity\nv_bar_max: maximal linear velocity\n\n\n\nCode\ndx, dy = (\n    goal_point[0] - self.current_pos[0],\n    goal_point[1] - self.current_pos[1],\n)\nabs_target_angle = math.atan2(dy, dx)\nturn_error = abs_target_angle - np.deg2rad(self.current_heading)\nL_d = math.sqrt(dx**2 + dy**2)\n\nR = np.abs(L_d / (2.0 * np.sin(turn_error)))\nv = np.clip(v_bar * R, v_bar_min, v_bar_max)\nomega = omega_factor * (width * np.sin(turn_error) * v) / L_d\n\n\nThe full code can be found here"
  },
  {
    "objectID": "index.html#results",
    "href": "index.html#results",
    "title": "Lane Following using Pure Pursuit Control",
    "section": "Results",
    "text": "Results\n\nPure Pursuit - “Follow the carrot”\nAlthought this approach is quite simple, we found that it performed relatively well with the right parameters. When driving at lower speed, the duckiebot was able to turn the corner gracefully without too much oscillation. However, as soon as we get it to drive a bit faster, it wasn’t very good at turning around corners, especially in the inner lane due to its sharp corners.\nOuter Loop:\n\nInner Loop:\n\n\n\nPure Pursuit - Tangent approach\nWith the tangent approach we can observe the duckiebot going faster on the straights and slower around the corners. However this approach did not produce strictly better results than the “follow the carrot” approach. As after turning around the corner, the tangent approach overcorrected way more than with the naive approach.\nSome of these results can be explained because we changed the map on which the duckie drove, which generated a worse trajectory than the test we had generated prior.\nOuter Loop:\n\nInner Loop:\n\n\n\nTrajectory Visualization\nWe also visualize the generated trajectory (shown in red points) along with the detected white and yellow bouundries in the two videos below.\nTrajectory Outer Loop:\n\nTrajectory Inner Loop:"
  },
  {
    "objectID": "index.html#testing",
    "href": "index.html#testing",
    "title": "Lane Following using Pure Pursuit Control",
    "section": "Testing",
    "text": "Testing\nTesting instructions are also available in the README of the project repository, here. They are also copied here for completeness.\nTo test the code, first clone the project GitHub repository.\ngit clone git@github.com:kumaradityag/fp-control\nThese instructions assume you have the following:\n\nA duckietown-shell. You can verify that you have sucessfully installed it using dts --version. If that’s not the case, please follow the instructions here\nYou have created your virtual duckiebot. You can verify that your virtual duckie exists with the command dts duckiebot virtual start vbot and then dts fleet discover. vbot is the name of our virtual duckie, but feel free to replace it with your own virtual duckie name\nYou have your real life duckiebot setup. This step is only required if you want to test the demo in real life. If you want to get your duckie hardware, see the official documentation here\n\n\nUnderstanding the project architecture\nThe most important directories of our projects are packages/trajectory_planner and packages/pure_pursuit_control, which are where our trajectory generation code and our pure pursuit code live.\nVirtual and physical duckiebot can behave slightly differently because of real-world physics, so they respond to the same parameters differently. Additionally, in our current setup, driving in the inner or outter lane also require different parameters. Therefore, you will probably need to tune your parameters for your own duckiebot.\nThe configs for the trajectory planner and the pure pursuit controller can be found respectively inside packages/trajectory_planner/config/trajectory_planner_node/default.yaml and packages/pure_pursuit_control/config/pure_pursuit_control_node/default.yaml. Additional config information is present at the bottom.\n\n\nSetup\nYou will need at least three terminals to spin and view everything needed. Replace &lt;duckie_name&gt; with your duckiebot name.\nPreliminaries (in every terminal):\n# Set robot name\nexport ROBOT_NAME=&lt;duckie_name&gt;\nPreliminaries for the matrix (if needed):\ndts matrix run --standalone --map ./assets/duckiematrix/map/loop/\ndts matrix attach $ROBOT_NAME map_0/vehicle_0\nAll these other commands need to run from the project repository:\nIn terminal 1:\ndts devel build -H $ROBOT_NAME -f\nIn terminal 2:\n# To view the debug images\ndts duckiebot image_viewer $ROBOT_NAME\nIn terminal 3:\n# For emergency stop\ndts duckiebot keyboard_control $ROBOT_NAME\nIn terminal 4:\n# If you want to manually change some of the param values\nssh duckie@&lt;duckie_name&gt;.local\n# After logging into your duckiebot:\ndocker exec -it ros-interface bash\nrosparam list | grep &lt;param_search_query&gt;\nrosparam set &lt;param_name&gt; &lt;value&gt;\n\nRun the carrot approach\nIn terminal 1:\n# For physical duckie\ndts devel run -H $ROBOT_NAME -L lane-following-carrot-physical-outer\ndts devel run -H $ROBOT_NAME -L lane-following-carrot-physical-inner\n\n# For virtual duckie\ndts devel run -H $ROBOT_NAME -L lane-following-carrot-virtual-outer\ndts devel run -H $ROBOT_NAME -L lane-following-carrot-virtual-inner\n\n\nRun the tangent approach\n# For physical duckie\ndts devel run -H $ROBOT_NAME -L lane-following-tangent-physical-outer\ndts devel run -H $ROBOT_NAME -L lane-following-tangent-physical-inner\n\n# For virtual duckie\ndts devel run -H $ROBOT_NAME -L lane-following-tangent-virtual-outer\ndts devel run -H $ROBOT_NAME -L lane-following-tangent-virtual-inner\n\n\n\nConfig Descriptions\nThe trajectory planner node has the following config parameters:\n\nmin_forward: minimum distance at which we consider trajectory points\nmax_forward: maximum distance at which we consider trajectory points\nn_samples: number of samples used for ransac\nlane_width: width of the lane in meters\nepsilon: error around lane width\nyellow_pts_threshold: minimum points in the trajectory for the yellow lane to be valid\nwhite_pts_threshold: minimum points in the trajectory for the white lane to be valid\ndefault_mode: relying on WHITE/YELLOW lane\nransac_max_iterations: number of iterations for ransac to compute inliers\nransac_distance_threshold: max error to separate inliers from outliers (in m)\npoly_degree: polynomial degree for ransac\nbuffer_size: number of previous trajectory saved in buffer\nbuffer_smooth_alpha: float number between 0-1 used to smooth out current trajectory based on the previous one\nbuffer_theta_threshold: theta angle threshold for heading change in degrees\n\nThe pure pursuit control node has the following config parameters:\n\nlookahead_distance: distance at which the pure pursuit controller look ahead (in cm)\nv_bar: linear velocity\nv_bar_max: maximal linear velocity\nv_bar_min: minimal linear velocity\nwidth: chassis width of the duckie. Should stay 0.1\nomega_factor: how hard we want the duckie to turn"
  },
  {
    "objectID": "index.html#conclusion",
    "href": "index.html#conclusion",
    "title": "Lane Following using Pure Pursuit Control",
    "section": "Conclusion",
    "text": "Conclusion\nThis project successfully implemented a modular lane-following pipeline for the Duckiebot, moving away from the state estimation approach in favor of explicit trajectory generation and geometric path tracking. By leveraging robust polynomial fitting on ground-projected lane segments, we created a system capable of generating a centerline trajectory and following it using pure pursuit control.\nOur experiments demonstrated that a pure pursuit controller, allows for continuous driving in both simulation and physical Duckietowns. While the transition from simulation to hardware introduced challenges regarding sensor noise and latency, the system proved that looking ahead at a generated path yields intuitive driving behavior. Our implementation also offers a flexible foundation for implementing more advanced planners and dynamic controllers in the future and general experimentation.\n\nLimitations\nAlthough our pipeline performed well in simulation and achieved successful loops in real-world tests, the gap between simulation and reality exposed some weaknesses in both our trajectory generation and control algorithms.\n\nTrajectory Generation Limitations\n\nSingle-Boundary Dependency: Currently, the system fits a curve to either the white or the yellow lane markings, but does not fuse both simultaneously. This view of the road geometry creates instability: if the chosen boundary becomes sparse (but still above a given threshold) or occluded, the fit degrades immediately, even if the other boundary is clearly visible. A robust system could fit both boundaries jointly to maximize data utilization.\nModel Constraints (The “S-Curve” Problem): We model the lane as a quadratic polynomial. While robust for simple turns, a parabola cannot model inflection points (S-curves). If the vision horizon extends through a left-then-right turn, the quadratic fit will average the two curves into a straight line, causing the robot to drive off-road. Testing with a cubic polynomial in these situations might prove useful.\n\n\n\nPure Pursuit Limitations\nDespite its computational efficiency, the pure pursuit controller exhibited some flaws when deployed on physical hardware:\n\nStatic Lookahead Sensitivity: The controller is rigid; a fixed lookahead distance cannot handle both sharp corners (requires short lookahead) and straightaways (requires long lookahead) effectively. Without an adaptive lookahead based on curvature or speed, the robot oscillates on straights or cuts corners too aggressively.\nUnmodeled Dynamics & Latency: Pure pursuit assumes instantaneous kinematic response. It fails to account for the Duckiebot’s system latency and tire slip at higher speeds. This sometimes results in a “reactive” driving style where the robot corrects errors only after they have accumulated, leading to jerky, oscillatory motion."
  }
]