[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lane Following using Pure Pursuit Control",
    "section": "",
    "text": "This project implements lane following on a Duckiebot using trajectory generation and pure pursuit control. The standard Duckietown lane-following stack estimates lane pose (d, \\phi) via a histogram filter and applies a PID-style controller on that estimate. In practice, the (d, \\phi) measurement can be noisy and discontinuous. Especially under partial occlusions, sparse lane markings, or during turns, which leads to jittery steering and reduced cornering smoothness.\nInstead of using (d, \\phi), we use perception output (ground-projected lane segments) directly for local path construction. At each time step, we generate a centerline trajectory in the robot frame, apply temporal smoothing, and track the resulting path using pure pursuit control. This replaces explicit lane-state estimation with a path-tracking formulation: control commands are computed directly from the local trajectory, yielding smoother driving through curves.\nThe baseline stack consists of:\n\nLine detection: color segmentation (white/yellow/red), edge detection (Canny), and line extraction (Hough) to produce lane-marker segments in the image.\nGround projection: homography-based mapping from image coordinates to the ground plane using camera calibration.\nLane estimation: a histogram filter over (d, \\phi) that fuses a kinematic prediction with segment-based updates.\nControl: PID-style steering using the estimated (d, \\phi), typically with constant forward velocity and angular correction based on lateral and heading error.\n\nWe keep line detection and ground projection, but replace the downstream modules:\n\nTrajectory generation: compute a forward centerline path from projected segments, with robust fitting and temporal smoothing.\nPure pursuit control: select a lookahead goal point on the path and compute curvature/steering commands to track it.\n\n\n\n\n\n\nflowchart LR\n  A[\"Camera Image\"] --&gt; B[\"Line Detection\"]\n  B --&gt; C[\"Ground Projection\"]\n\n  subgraph Baseline_Duckietown [\"Baseline Duckietown Stack\"]\n    D[\"Lane Estimation\"] --&gt; E[\"PID Control\"]\n  end\n\n  subgraph Our_Project [\"Our Project Stack\"]\n    F[\"Trajectory Generation\"] --&gt; G[\"Pure Pursuit Control\"]\n  end\n\n  C --&gt; D\n  C --&gt; F\n  E --&gt; H[\"Wheel Commands\"]\n  G --&gt; H\n\n\n\n\n\n\nOver this project we set and achieved these three goals:\n\nTrajectory generation from detected lane segments, including temporal smoothing for stability.\nPure pursuit control on the generated trajectory.\nDemonstrate continuous looped driving on a real Duckiebot in the lab Duckietown."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Lane Following using Pure Pursuit Control",
    "section": "",
    "text": "This project implements lane following on a Duckiebot using trajectory generation and pure pursuit control. The standard Duckietown lane-following stack estimates lane pose (d, \\phi) via a histogram filter and applies a PID-style controller on that estimate. In practice, the (d, \\phi) measurement can be noisy and discontinuous. Especially under partial occlusions, sparse lane markings, or during turns, which leads to jittery steering and reduced cornering smoothness.\nInstead of using (d, \\phi), we use perception output (ground-projected lane segments) directly for local path construction. At each time step, we generate a centerline trajectory in the robot frame, apply temporal smoothing, and track the resulting path using pure pursuit control. This replaces explicit lane-state estimation with a path-tracking formulation: control commands are computed directly from the local trajectory, yielding smoother driving through curves.\nThe baseline stack consists of:\n\nLine detection: color segmentation (white/yellow/red), edge detection (Canny), and line extraction (Hough) to produce lane-marker segments in the image.\nGround projection: homography-based mapping from image coordinates to the ground plane using camera calibration.\nLane estimation: a histogram filter over (d, \\phi) that fuses a kinematic prediction with segment-based updates.\nControl: PID-style steering using the estimated (d, \\phi), typically with constant forward velocity and angular correction based on lateral and heading error.\n\nWe keep line detection and ground projection, but replace the downstream modules:\n\nTrajectory generation: compute a forward centerline path from projected segments, with robust fitting and temporal smoothing.\nPure pursuit control: select a lookahead goal point on the path and compute curvature/steering commands to track it.\n\n\n\n\n\n\nflowchart LR\n  A[\"Camera Image\"] --&gt; B[\"Line Detection\"]\n  B --&gt; C[\"Ground Projection\"]\n\n  subgraph Baseline_Duckietown [\"Baseline Duckietown Stack\"]\n    D[\"Lane Estimation\"] --&gt; E[\"PID Control\"]\n  end\n\n  subgraph Our_Project [\"Our Project Stack\"]\n    F[\"Trajectory Generation\"] --&gt; G[\"Pure Pursuit Control\"]\n  end\n\n  C --&gt; D\n  C --&gt; F\n  E --&gt; H[\"Wheel Commands\"]\n  G --&gt; H\n\n\n\n\n\n\nOver this project we set and achieved these three goals:\n\nTrajectory generation from detected lane segments, including temporal smoothing for stability.\nPure pursuit control on the generated trajectory.\nDemonstrate continuous looped driving on a real Duckiebot in the lab Duckietown."
  },
  {
    "objectID": "index.html#trajectory-generation",
    "href": "index.html#trajectory-generation",
    "title": "Lane Following using Pure Pursuit Control",
    "section": "Trajectory Generation",
    "text": "Trajectory Generation\nAdd info about trajectory generation\nWe can place images or charts that span the full width or sit in the margin.\n\n\n\n\n\nA smaller figure ref for now\n\n\n\nCode Block\nBecause we enabled code folding, this big block won’t clutter the page unless the user clicks “Show Code”.\n\n\nCode\nimport numpy as np\n\na = 1\nb = 2\nmultiple = a * b\n\n# Complex plotting code..."
  },
  {
    "objectID": "index.html#pure-pursuit-control",
    "href": "index.html#pure-pursuit-control",
    "title": "Lane Following using Pure Pursuit Control",
    "section": "Pure Pursuit Control",
    "text": "Pure Pursuit Control\nThe second part of the project focused on enhancing the controller node using the “Pure Pursuit” algorithm, which enables the robot to adjust its path before the error accumulates. Similar to PID control, pure pursuit is a steering method that computes the linear (v) and angular velocity (\\omega). However, instead of relying on the cross-track error (CTE), the lateral distance between the vehicle and the centerlane, it relies on the pre-computed trajectory as a reference.\nThe pure pursuit controller consists of two keys steps:\n\nGoal Point Computation: We determine the goal point to which the robot aims to reach\nControl Commands Computation: We compute the linear and angular velocity required to make the duckie reach the computed goal point\n\n\n\n\n\n\nflowchart LR\n    A[Goal Point Computation] --&gt; B[Control Command Computation]\n\n\n\n\n\n\nIf you are familiar with the “carrot and stick” analogy, pure pursuit control works fashionably in the same way: we make the robot (the donkey) move towards the goal point (the carrot), which we always keep at a distance L_d, the lookahead distance. If the donkey is too far from the goal point, we “stick” it to the robot and make it turn more aggressively towards the goal point.\n\n\nGoal Point Calculation - Line-circle intersection algorithm\nMany methods can be used to compute the goal point, but we decided to go with the “line-circle intersection” algorithm for its computational simplicity.\nMathematically speaking, the line-circle intersection algorithm tries to find the points where a straight line intersects the circle with radius R.\n\n\n\n\n\n\nNoteUnderstanding the maths behind the line-circle intersection algorithm\n\n\n\n\n\nGiven a circle centered at the origin with radius r and points P1(x1, y1) and P2(x2, y2). The implicit line equation for the closed-form geometry can be written as\n d_y x - d_x y + D = 0\nwhere\n\nD=x_1 y_2 - x_2 y_1\nd_x = x_2 - x_1\nd_y = y_2 - y_1\n\nThe perpendicular projection of the origin onto the line is:\n\n\\begin{aligned}\nx_0 &= \\frac{D d_y}{d_r^2} \\\\\ny_0 &= \\frac{-D d_x}{d_r^2}\n\\end{aligned}\n\nwhere dr= \\sqrt{dx^2 + dy^2}\nSince the closest point from the origin to the line is a perpendicular projection, we can determine the existence of the intersection using the distance from the origin to the line\n\\text{dist} = \\frac{|D|}{d_r}\n\nIf dist &gt; r, there are no intersection\nIf dist = r, one tangent intersection\nIf dist &lt; r, there are two intersections\n\nFrom the right-triangle geometry, we can find the distance along the line from the closest point to each intersection, which we can use to move from the closest point forward and backward along the line\nh = \\sqrt{r^2 - \\left(\\frac{D}{d_r}\\right)^2}\nThe unit direction along the line is given by\n\\hat{\\mathbf{u}} = \\left( \\frac{d_x}{d_r}, \\frac{d_y}{d_r} \\right)\nThis gives us the formula to compute the intesection points\n\n\\mathbf{P}_{1,2}\n=\n\\begin{pmatrix}\nx_0 \\\\\ny_0\n\\end{pmatrix}\n\\pm\nh \\,\\hat{\\mathbf{u}}\n\n\n\n\n\n\n\n\n\n\n\\begin{aligned}\nx &= \\frac{D d_y \\pm d_x \\sqrt{r^2 d_r^2 - D^2}}{d_r^2} \\\\\ny &= \\frac{-D d_x \\pm d_y \\sqrt{r^2 d_r^2 - D^2}}{d_r^2}\n\\end{aligned}\n\n\n\n\nSince the line-circle intersection method works for two points only and the trajectory is an array of points, we need to iteratively compute the potential intersections points for contiguous segments as follows\n\nx=\\frac{D dy \\pm sgn(dy) dx \\sqrt{\\Delta}}{L_d^2}\ny=\\frac{-D dx \\pm \\| dy \\| \\sqrt{\\Delta}}{L_d^2}\n\nWhere\n\ndx=x_2-x_1\ndy=y_2-y_1\ndr= \\sqrt{dx^2 + dy^2}\nD=x_1 y_2 - x_2 y_1\n\\Delta=r^2 dr^2 - D^2\nf(x) = \\begin{cases} 0, & x&lt;0 \\\\ x, & x\\ge 0 \\end{cases}\n\nTo determine the validity of the goal point computed, we can follow the following graph:\n\n\n\n\n\nflowchart LR\n    A[Discriminant] -- $$\\Delta$$&lt;0 --&gt; B(No intersection Found) --&gt; E;\n    A -- $$\\Delta$$=0 --&gt; C(One Intersection Found);\n    A -- $$\\Delta$$&gt;0 --&gt; D(Two Intersections Found);\n    E[Use last point in trajectory];\n    F(Range Check);\n    G[Select intersection closest to last goal point];\n    C -- invalid --&gt; E;\n    D -- both invalid --&gt; E;\n    D -- one invalid --&gt; C;\n    D -- both valid --&gt; G;\n\n\n\n\n\n\nEssentially, we compute the discriminant \\Delta to find valid intersections. The intersection we find is valid if \\Delta \\ge 0 and if it’s between the trajectory segment points.\nThe full implementation can be found inside the function find_goal_point()\n\n\nControl Command Computation - “Follow the carrot” approach (naive)\nThe first implementation of pure pursuit that we coded kept the linear velocity constant and only accounted for the angular velocity, which was calculated using the turn error between the robot heading and the goal point.\nWe define 3 parameters:\n\nlookahead_distance: Circle radius at which the duckiebot sees\nkp: How hard do we want to steer the wheel upon turn error\nv_bar: Linear velocity\n\nConsider the following picture,\n\nThe turn error can be computed\n\\alpha = tan(\\frac{y_1-y_0}{x_1 - x_0})\nThus,\n\\omega = kp \\cdot \\alpha\nThe code for the “carrot and stick” approach looks something like this:\n\n\nCode\ngoal_point, _ = find_goal_point(\n    path_points,\n    self.current_pos,\n    lookahead_distance,\n    self.last_found_index,\n)\ndx, dy = (\n    goal_point[0] - self.current_pos[0],\n    goal_point[1] - self.current_pos[1],\n)\nabs_target_angle = math.atan2(dy, dx)\nturn_error = abs_target_angle - np.deg2rad(self.current_heading)\nL_d = math.sqrt(dx**2 + dy**2)\n\nv = v_bar\nomega = kp * turn_error\n\n\nThe full code implementation can be found in the function compute_control_action()\n\nResults - “Follow the carrot”\nAlthought this approach is quite simple, we found that it performed relatively well with the right parameters. When driving at lower speed, the duckiebot was able to turn the corner gracefully without too much oscillation. However, as soon as we instruct it to drive a bit faster, it wasn’t very good at turning around corners, especially in the inner lane due to its sharp corners.\nOuter Loop:\n\nInner Loop:\n\n\n\n\nControl Command Computation - Curvature-based approach\nIn order to improve the duckiebot performance around sharp corners, we wanted to adjust the linear velocity and angle velocity based on curvature. Similarly to how one would drive, we want our duckie to slow down around corner and speed up on when it drives straight ahead. To do so, we used a tangent based approach to find the curvature.\nConsider the following picture:\n\n\n\ncourtesy of Purdue Sigbots\n\n\nUsing trigonometry, we find that\nR = \\frac{L_d}{2 sin(\\alpha)}\n\n\n\n\n\n\nNoteDeriving the radius formula\n\n\n\n\n\nAs seen previously, given the current pos P0(x0, y0) and the goal point P1(x1, y1), the turn error \\alpha can be computed as\n\\alpha = tan(\\frac{y_1 - y_0}{x_1 - x_0})\nSince the goal point is at distance L_d of the current position, the half-way point is at distance \\frac{L_d}{2} and thus\nR = \\frac{\\frac{L_d}{2}}{sin(\\alpha)} = \\frac{L_d}{2 sin (\\alpha)}\n\n\n\nThe curvature k is defined as the inverse of the radius of the arc we want the duckie to travel. Because we want the linear velocity to be inversely proportional to the curvature (the higher the curvature, the lower the speed), we get the following formula for linear velocity\nk = \\frac{1}{R} v = \\bar{v} * \\frac{1}{k} = \\bar{v} * R\nTo derive the angular error, now consider the diagram below:\n\n\n\ncourtesy of Purdue Sigbots\n\n\nAssuming that the robot finishes the turn after time \\delta t, we have that the left side (L_l) and right side (L_R) of the duckie turn with velocity\n\n\\begin{aligned}\nL_l\n&=\n(\\text{linearVel} - \\text{turnVel}) \\cdot \\Delta t\n=\n\\left( R - \\frac{W}{2} \\right) \\cdot (2 \\cdot \\text{turnError})\n\\\\[6pt]\nL_r\n&=\n(\\text{linearVel} + \\text{turnVel}) \\cdot \\Delta t\n=\n\\left( R + \\frac{W}{2} \\right) \\cdot (2 \\cdot \\text{turnError})\n\\end{aligned}\n\nSolving for turn velocity, we get that\n\n\\frac{(\\text{linearVel} - \\text{turnVel}) \\cdot \\Delta t}\n     {(\\text{linearVel} + \\text{turnVel}) \\cdot \\Delta t}\n=\n\\frac{\\left( R - \\frac{W}{2} \\right) \\cdot (2 \\cdot \\text{turnError})}\n     {\\left( R + \\frac{W}{2} \\right) \\cdot (2 \\cdot \\text{turnError})}\n\n\n\\frac{\\text{linearVel} - \\text{turnVel}}\n     {\\text{linearVel} + \\text{turnVel}}\n=\n\\frac{R - \\frac{W}{2}}\n     {R + \\frac{W}{2}}\n\n\n\\left( R + \\frac{W}{2} \\right)\\text{linearVel}\n-\n\\left( R + \\frac{W}{2} \\right)\\text{turnVel}\n=\n\\left( R - \\frac{W}{2} \\right)\\text{linearVel}\n+\n\\left( R - \\frac{W}{2} \\right)\\text{turnVel}\n\n\n\\text{turnVel}\n=\n\\frac{W}{2R} \\cdot \\text{linearVel}\n\nAnd since R = \\frac{\\text{lookaheadDistance}}{2 \\sin(\\text{turnError})}, the turn velocity\n\n\\boxed{\n\\text{turnVel}\n=\n\\frac{W \\sin(\\text{turnError})}\n     {\\text{lookaheadDistance}}\n\\cdot \\text{linearVel}\n}\n\nThus, our curvature-based pure pursuit approach uses 4 parameters:\n\nwidth: width of the duckiebot chassis\nomega_factor: how hard we want to turn the steering wheel, very similar to kp\nv_bar: default velocity\nv_bar_min: minimal linear velocity\nv_bar_max: maximal linear velocity\n\n\n\nCode\ndx, dy = (\n    goal_point[0] - self.current_pos[0],\n    goal_point[1] - self.current_pos[1],\n)\nabs_target_angle = math.atan2(dy, dx)\nturn_error = abs_target_angle - np.deg2rad(self.current_heading)\nL_d = math.sqrt(dx**2 + dy**2)\n\nR = np.abs(L_d / (2.0 * np.sin(turn_error)))\nv = np.clip(v_bar * R, v_bar_min, v_bar_max)\nomega = omega_factor * (width * np.sin(turn_error) * v) / L_d\n\n\nThe full code can be found here\n\nResults - “Tangent Approach”\nTheoritically, the tangent approach should have yielded better results than the “carrot and stick” approach, however, this wasn’t what we observed. After turning around the corner, the tangent approach overcorrected way more than with the naive approach. Additionally, we see that the robot didn’t correct cross-track error and drove near some time on the line before correcting for it.\nOuter Loop:\n\nInner Loop:\n\nSome of these results can be explained because we changed the map on which the duckie drove, which generated a worse trajectory than the test we had generated prior.\nTrajectory Outer Loop:\n\nTrajectory Inner Loop:"
  },
  {
    "objectID": "index.html#results",
    "href": "index.html#results",
    "title": "Lane Following using Pure Pursuit Control",
    "section": "Results",
    "text": "Results\nAdd result videos to this section."
  },
  {
    "objectID": "index.html#testing",
    "href": "index.html#testing",
    "title": "Lane Following using Pure Pursuit Control",
    "section": "Testing",
    "text": "Testing\nTo test the code, first clone the project GitHub repository and follow the steps defined in the README.md file.\ngit clone git@github.com:kumaradityag/fp-control\nIf you followed all the steps in the README.md file, you should have the following: - You have cloned the repo - You have installed the duckietown-shell. You can verify that you have sucessfully installed it using dts --version. If that’s not the case, please follow the instructions here - You have created your virtual duckiebot. You can verify that your virtual duckie exists with the command dts duckiebot virtual start vbot and then dts fleet discover. vbot is the name of our virtual duckie, but feel free to replace it with your own virtual duckie name - You have your real life robot. This step is only required if you want to test the demo in real life. If you want to get your duckie hardware, see the official documentation here\n\nUnderstanding the project architecture\nThe most important directories of our projects are packages/trajectory_planner and packages/pure_pursuit_control, which are where our trajectory generation code and our pure pursuit code live.\n\n\nTesting the project\nVirtual and physical duckie have different wheel friction and mass distribution, so they respond to the same parameters differently. Additionally, driving in the inner or outter lane also require different parameters. Therefore, you will probably need to tune your parameters for your own duckiebot.\nThe configs for the trajectory planner and the pure pursuit controller can be found respectively inside packages/trajectory_planner/config/trajectory_planner_node/default.yaml and packages/pure_pursuit_control/config/pure_pursuit_control_node/default.yaml.\nThe trajectory planner node has the following parameters: - min_forward: minimum distance at which we consider trajectory points - max_forward: maximum distance at which we consider trajectory points - n_samples: number of samples used for ransac - lane_width: width of the lane in meters - epsilon: error around lane width - yellow_pts_threshold: minimum points in the trajectory for the yellow lane to be valid - white_pts_threshold: minimum points in the trajectory for the white lane to be valid - default_mode: relying on WHITE/YELLOW lane - ransac_max_iterations: number of iterations for ransac to compute inliers - ransac_distance_threshold: how far should ransac sample the points from (in m) - poly_degree: polynomial degree for ransac - buffer_size: number of previous trajectory saved in buffer - buffer_smooth_alpha: float number between 0-1 used to smooth out current trajectory based on the previous one - buffer_theta_threshold: theta angle for which in degrees\nThe pure pursuit control node has the following parameters: - lookahead_distance: distance at which the pure pursuit controller look ahead (in cm) - v_bar: linear velocity - v_bar_max: maximal linear velocity - v_bar_min: minimal linear velocity - width: chassis width of the duckie. Should stay 0.1 - omega_factor: how hard we want the duckie to turn\n\nLaunching the code\nOnce the parameters have been defined, the code can be launched with the following commands. You will need 4 terminals to view everything needed.\nIn terminal 1:\ncd fp-control\ndts fleet discover\ndts matrix run --standalone --map ./assets/duckiematrix/map/loop/\nIn terminal 2:\ndts matrix attach vbot map_0/vehicle_0\ndts devel build -H vbot -f\ndts devel run -H vbot -L lane-following\nIn terminal 3:\ndts gui vbot\n# Wait for the entrypoint - inside it run:\nrqt_image_view\n# Go to /trajectory_planner to see generated trajectory\nIn terminal 4:\nssh duckie@vbot.local\n# After logging into your duckiebot:\ndocker exec -it ros-interface bash\nSome tips: - Instead of re-building the code every time we change the parameters in terminal 2, we can use the command rosparam set &lt;param_name&gt; &lt;value&gt;. The list of params can be found with rosparam list | grep &lt;param_name&gt; - To override the duckie commands, you can open a fifth terminal and use the keyboard control with dts duckiebot keyboard_control vbot. To make the duckie stop, you can click on “emergency stop”.\n\n\nTesting the project on the virtual duckie\nParameters to drive on the outer lane\nTBD\nParameters to drive on the inner lane\nTBD\n\n\nTesting the project on the physical duckie\nParameters to drive on the outer lane\nTBD\nParameters to drive on the inner lane\nTBD"
  },
  {
    "objectID": "index.html#conclusion",
    "href": "index.html#conclusion",
    "title": "Lane Following using Pure Pursuit Control",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "index.html#limitations",
    "href": "index.html#limitations",
    "title": "Lane Following using Pure Pursuit Control",
    "section": "Limitations",
    "text": "Limitations\nAlthough our pure pursuit method performed quite well in simulation, its performance in the physical world has substantial room for improvement. The gap between simulation and reality exposed fundamental weaknesses in both our trajectory generation and control algorithms.\n\nTrajectory Generation Limitations\nOur trajectory generation is still not robust to sensor noise and environmental variability in the real world. This fragility becomes evident when comparing the real-world performance on the closed inner-loop map and the intersection map. While the duckiebot provided consistent lane boundaries in the virtual environment, our outlier rejection mechanism failed to adequately filter out incorrect lane boundaries introduced by lightning variation and occlusions that occur in the real world, making the trajectory generation algorithm include incorrect boundary detection into its trajectory generation.\nAdditionally, we saw that the RANSAC line fitting algorithm performance is highly dependent on the parameters being given. The inlier and yellow points threshold required manual tuning for each distinct scenario: the parameters proved effective for inner-loop failed on the outer-loop, forcing us to maintain distinct parameter configuration for different maps, making this approach impractical when dealing with unknown environments. A particularly challenging issue emerged with our inner-loop vs outer-loop boundary selection strategy. Currently, we rely on the inner-boundary (yellow) when driving on the inner lane whereas we rely on the outer-boundary (white) when driving on the outer lane, but real-world scenarios demand dynamic boundary selection. An ideal solution would be to select the most trustworthy boundary based on some quality metrics.\n\n\nPure Pursuit Limitations\nDespite its computationally efficient, the pure pursuit controller’s performance is highly sensitive to the lookahead distance parameter and greatly impacts our system behavior. Insufficient lookahead distance induces oscillations and overesteering response around minor errors whereas excessive lookahead distance causes aggressive corner-cutting. An adaptive lookahead strategy, where lookahead would be adjusted based on vehicle speed, path curvature and cross-track error performance would address this rigid approach.\nMore importantly, pure pursuit fundamental flaws lie in its assumption of instantaneous command execution. While this assumption remains somewhat true in simulation, this premise crumbles when the duckie enters the physical world. The robot exhibits non-negligible actuators delay (around 150-200ms from command to execution) and doesn’t account for tire slip that may occur at moderate speed. When faced with sharp maneuvers, the vehicle’s delayed response accumulates tracking error, prompting the controller to correct itself aggressively. This feedback pattern produces jerky driving behavior, which wouldn’t occur in an experienced human driver.\nOur attempt at a dynamic control strategy through curvature-based velocity modulation only saw marginal improvements: the duckiebot continued to exhibit harsh braking around tight corners and aggressive acceleration upon exit. This nauseating driving behavior can be explained by the controller’s single optimization objective: minimizing its geometric deviation from the reference path. The controller possesses no notion of passenger comfort and doesn’t adhere to predictable driving norms, which are all behavior needed to drive safely on the road."
  },
  {
    "objectID": "index.html#whats-next",
    "href": "index.html#whats-next",
    "title": "Lane Following using Pure Pursuit Control",
    "section": "What’s next",
    "text": "What’s next\nWhile our project successfully demonstrated lane-following in a closed loop environment, numerous opportunities exist to extend this foundation toward more robust and sophisticated autonomous navigation. We identified several directions for future development:\n\nImprove Lane Boundaries Detection\nWe have seen that the current lane boundary detection is still not robust to outliers due to occlusion and lightning variation. Several strategies could be implemented to improve robustness. Temporal filtering could be leveraged to exploit the sequential nature of video suppress transient noise across consecutive frames. Region of interest masking could also be applied to constraint detection only around the region where lanes are expected to appear, which would reduce false positives coming from ceiling and irrelevant background elements. Most importantly, Bayesian filtering such as Kalman filters could be applied to maintain a probabilistic belief for the lane positions and orientation over time. This approach would enable the duckie to “remember” the lane boundaries during a brief occlusion.\n\n\nImprove Trajectory Generation\nThe current RANSAC-based approach only minimize geometrical error and doesn’t account for jerky trajectories. Quadratic programming (QP) could be implemented to minimize for both the geometrical error and jerk behavior by defining constraints around velocity, acceleration and curvature limits."
  },
  {
    "objectID": "index.html#improving-control-architecture",
    "href": "index.html#improving-control-architecture",
    "title": "Lane Following using Pure Pursuit Control",
    "section": "Improving Control Architecture",
    "text": "Improving Control Architecture\nPure pursuit works relatively well when we need to control around corner, but only takes into account heading error for more stable control. A naive improvement would be to include both PID and pure pursuit idea and include both cross-track error and heading error, something which Stanley control does quite well. Theoretically, Stanley control would be more stable at higher and lower speed because it would naturally transition between behaviors based on velocity: the heading term would dominate on higher speed and would gradually transition to CTE as it slows down.\n\nObject Detection and Avoidance\nImplementing pedestrian detection and integrating dynamic obstacle avoidance would be the first step toward context-aware navigation. Pedestrian detection could be solved using modern deep learning architecture such as YOLO or MobileNet while dynamic obstacle avoidance could be implemented using the Dynamic Window Approach (DWA). DWA samples commands and simulate their execution over a short horizon and selects the command which maximize progress toward the goal point while keeping the trajectory collision-free.\n\n\nIntersection Navigation\nThe second step towards context-aware navigation is to make the duckiebot drive around maps with intersections and introduces new challenges: stop line detection and turn signal interpretation. Successful behavior planning requires multi-stage hierarchical maneuver planning in order to plan what the duckie needs to do next: stop at the line, wait for traffic to clear, turn right, change lane, …\n\n\nMulti-Agent Coordination\nThe last step towards context-aware navigation is to add coordination inside the system, that is to make the duckiebot drive around other vehicles. Coordination requires a communication architecture for specific situations, mainly around intersection and traffic light. Simple heuristics around common driving courtesy (no hard breaks, FIFO at 4 points intersection, …) could first be implemented and then decentralized coordination second."
  }
]